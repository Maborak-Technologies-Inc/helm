# Default values for amazon-watcher-stack
# This is a YAML-formatted file.

nameOverride: "apt"
fullnameOverride: ""

# Global configuration
global:
  context: amazon-watcher
  domain:
    ui: ui.amazon-watcher.local
    backend: api.amazon-watcher.local
  # Global HPA override (empty = use individual service settings, true/false = override all)
  hpa: ""  # Options: "" (use individual), true (enable HPA for all), false (disable HPA for all)
  # Global persistent storage (shared across backend, CLI jobs, and other services)
  storage:
    enabled: true  # Enable global persistent storage
    storageClassName: ""  # Storage class (empty = use default)
    size: 10Gi  # Storage size
    accessModes:
      - ReadWriteOnce  # Options: ReadWriteOnce, ReadWriteMany (for concurrent access)

# Database Configuration
database:
  enabled: true
  postgres:
    version: "16-alpine"
    db: amazon_watcher
    user: amazon_watcher
    password: amazon_watcher_password
    port: 5432
    dataPath: /var/lib/postgresql/data/pgdata
  storage:
    storageClassName: ""
    size: 10Gi
    accessModes:
      - ReadWriteOnce
  resources:
    limits:
      memory: 1Gi
      cpu: 1000m
    requests:
      memory: 512Mi
      cpu: 500m
  healthCheck:
    enabled: true
    intervalSeconds: 5
    timeoutSeconds: 5
    failureThreshold: 5
    initialDelaySeconds: 10
  # Network Policy configuration
  networkPolicy:
    enabled: true  # Set to true to enable NetworkPolicy
    # ingress: []  # Custom ingress rules (if not provided, defaults are used)
    # egress: []  # Custom egress rules (if not provided, defaults are used)

# Backend Service Configuration
backend:
  enabled: true
  replicas: 3
  # CLI Rollout configuration (for continuous monitoring/processing)
  cli:
    enabled: false  # Set to true to enable CLI Rollout
    replicas: 1  # Number of replicas (used when HPA is disabled)
    command: "python cli.py monitor run --batch-limit=50 --batch-interval=1 --cooldown-limit=5 --cooldown-duration=5 --country=BO --short"  # Command to run continuously
    waitForBackend: true  # Wait for backend service to be ready before starting
    maxWaitSeconds: 300  # Maximum seconds to wait for backend (default: 5 minutes)
    # Autoscaling configuration for CLI Rollout
    autoscaling:
      enabled: false  # Set to true to enable HPA for CLI
      minReplicas: 3  # Minimum number of CLI pods (HPA will scale down to this)
      maxReplicas: 10  # Maximum number of CLI pods (HPA will scale up to this)
      targetCPUUtilizationPercentage: 80  # Target CPU utilization percentage
      # targetMemoryUtilizationPercentage: 80  # Optional: Target memory utilization percentage
      # behavior: {}  # Optional: Custom scaling behavior (scaleUp/scaleDown policies)
    # Rollout configuration for CLI
    rollout:
      revisionHistoryLimit: 10  # Number of old ReplicaSets to retain
      progressDeadlineSeconds: 1800  # 30 minutes timeout for rollout progress
      # Strategy for sequential pod startup (one pod at a time)
      strategy:
        canary:
          # Sequential pod creation: only 1 new pod at a time
          maxSurge: 1  # Maximum number of pods that can be created above desired replicas
          maxUnavailable: 0  # No pods terminated until new one is ready
          steps:
            # Wait 30 seconds between each pod creation
            - pause:
                duration: 30  # Wait 30 seconds before proceeding to next pod
    # PodDisruptionBudget configuration for CLI
    podDisruptionBudget:
      enabled: false  # Set to true to enable PDB for CLI
      minAvailable: 2  # Minimum number of pods that must be available during disruptions
      # maxUnavailable: 1  # Alternative: Maximum number of pods that can be unavailable
  # CronJob configuration (for periodic tasks)
  cronjob:
    enabled: true  # Set to true to enable CronJob
    schedule: "* * * * *"  # Run every 10 minutes (cron format)
    command: "python cli.py system network ips"  # Command to run periodically
    waitForBackend: true  # Wait for backend service to be ready before running
    maxWaitSeconds: 300  # Maximum seconds to wait for backend (default: 5 minutes)
    concurrencyPolicy: Forbid  # Prevent concurrent job executions (Forbid, Allow, Replace)
    successfulJobsHistoryLimit: 3  # Number of successful jobs to keep
    failedJobsHistoryLimit: 1  # Number of failed jobs to keep
    suspend: false  # Set to true to suspend the CronJob
    backoffLimit: 3  # Number of retries before marking job as failed
    activeDeadlineSeconds: 600  # Maximum seconds a job can run (default: 10 minutes)
    resources:
      limits:
        memory: 512Mi
        cpu: 500m
      requests:
        memory: 256Mi
        cpu: 100m
    rollout:
      revisionHistoryLimit: 10  # Number of old ReplicaSets to retain
      progressDeadlineSeconds: 1800  # 30 minutes timeout for rollout progress
      # strategy: {}  # Optional: canary or blueGreen strategy
    # Autoscaling configuration
    autoscaling:
      enabled: false  # Enable Horizontal Pod Autoscaler
      minReplicas: 3  # Minimum number of replicas
      maxReplicas: 10  # Maximum number of replicas
      targetCPUUtilizationPercentage: 80  # Target CPU utilization
      # targetMemoryUtilizationPercentage: 80  # Optional: Target memory utilization
      # behavior: {}  # Optional: Scaling behavior configuration
    # Pod Disruption Budget configuration
    podDisruptionBudget:
      enabled: true  # Set to true to enable PDB
      # minAvailable: 2  # Minimum number of pods that must be available (mutually exclusive with maxUnavailable)
      maxUnavailable: 1  # Maximum number of pods that can be unavailable
  image:
    repository: maborak/platform
    tag: "apt-backend-0.1"
    pullPolicy: Always
  resources:
    limits:
      memory: 1Gi
      cpu: 1000m
    requests:
      memory: 512Mi
      cpu: 500m
  service:
    type: ClusterIP  # Options: ClusterIP, NodePort, LoadBalancer
    # nodePort: 30090  # Only used if type is NodePort
  healthCheck:
    enabled: true
    path: /health
    port: 9000
    # Readiness: Fast checks. If it fails, cut traffic immediately.
    readinessProbe:
      initialDelaySeconds: 15
      periodSeconds: 5
      timeoutSeconds: 5
      failureThreshold: 2
    # Liveness: Patient checks. Only restart if truly stuck.
    livenessProbe:
      initialDelaySeconds: 60
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 5
    # Legacy flat structure support (optional, can be removed if strict structure is preferred)
    initialDelaySeconds: 40
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3
  # Backend environment variables (APT_BACKEND_* prefix)
  # All env vars with full names including prefix - no transformation needed
  env:
    APT_BACKEND_DATABASE_URL: ""  # External database URL
    APT_BACKEND_DB_USE_REPLICA_ENGINE: "false"
    APT_BACKEND_DB_READ_URL: ""  # Read replica database URL (optional, for read-only queries)
    APT_BACKEND_DB_WRITE_URL: ""  # Write database URL (optional, for write operations)
    APT_BACKEND_DB_ECHO: "false"
    APT_BACKEND_DB_ECHO_POOL: "false"
    APT_BACKEND_DB_POOL_SIZE: "20"
    APT_BACKEND_DB_MAX_OVERFLOW: "30"
    APT_BACKEND_DB_POOL_TIMEOUT: "30"
    APT_BACKEND_DB_POOL_RECYCLE: "3600"

    APT_BACKEND_DB_READ_POOL_SIZE: "10"  # Read replica connection pool size
    APT_BACKEND_DB_WRITE_POOL_SIZE: "20"  # Write database connection pool size
    APT_BACKEND_UVI_HOST: "0.0.0.0"
    APT_BACKEND_UVI_PORT: "9000"
    APT_BACKEND_RELOAD: "false"
    APT_BACKEND_LOG_LEVEL: "info"
    APT_BACKEND_DEBUG_MODE: "false"
    APT_BACKEND_UVI_ACCESS_LOG: "true"
    APT_BACKEND_UVI_WORKERS: "5"
    APT_BACKEND_UVI_LIMIT_CONCURRENCY: "100"
    APT_BACKEND_UVI_LIMIT_MAX_REQUESTS: "1000"
    APT_BACKEND_UVI_TIMEOUT_KEEP_ALIVE: "30"
    APT_BACKEND_UVI_TIMEOUT_GRACEFUL_SHUTDOWN: "30"
    APT_BACKEND_JWT_ALGORITHM: "HS256"
    APT_BACKEND_JWT_ACCESS_TOKEN_EXPIRY: "3600"
    APT_BACKEND_JWT_REFRESH_TOKEN_EXPIRY: "2592000"
    APT_BACKEND_CORS_ORIGINS: "*"
    APT_BACKEND_CORS_ALLOW_CREDENTIALS: "true"
    APT_BACKEND_CORS_ALLOW_METHODS: "*"
    APT_BACKEND_CORS_ALLOW_HEADERS: "*"
    APT_BACKEND_CORS_EXPOSE_HEADERS: ""
    APT_BACKEND_CORS_MAX_AGE: "600"
    APT_BACKEND_RATE_LIMIT_ENABLED: "true"
    APT_BACKEND_RATE_LIMIT_REQUESTS: "20"
    APT_BACKEND_RATE_LIMIT_WINDOW: "60"
    APT_BACKEND_RATE_LIMIT_BYPASS_KEY: ""
    APT_BACKEND_RATE_LIMIT_EXCLUDED_PATHS: "/docs,/redoc,/openapi.json,/favicon.ico,/health"
    APT_BACKEND_RATE_LIMIT_BYPASS_PATHS: ""
    # Rate limiting for authentication endpoints
    APT_BACKEND_RATE_LIMIT_AUTH_LOGIN_MAX_REQUESTS: "200"
    APT_BACKEND_RATE_LIMIT_AUTH_LOGIN_WINDOW: "60"
    APT_BACKEND_RATE_LIMIT_AUTH_REGISTER_MAX_REQUESTS: "200"
    APT_BACKEND_RATE_LIMIT_AUTH_REGISTER_WINDOW: "60"
    APT_BACKEND_RATE_LIMIT_AUTH_VERIFY_MAX_REQUESTS: "500"
    APT_BACKEND_RATE_LIMIT_AUTH_VERIFY_WINDOW: "3600"
    APT_BACKEND_RATE_LIMIT_AUTH_PASSWORD_RESET_MAX_REQUESTS: "500"
    APT_BACKEND_RATE_LIMIT_AUTH_PASSWORD_RESET_WINDOW: "300"
    APT_BACKEND_RATE_LIMIT_AUTH_REFRESH_MAX_REQUESTS: "400"
    APT_BACKEND_RATE_LIMIT_AUTH_REFRESH_WINDOW: "60"
    APT_BACKEND_MONITOR_ENABLED: "true"
    APT_BACKEND_MONITOR_INTERVAL: "3600"
    APT_BACKEND_MONITOR_BATCH_SIZE: "5"
    APT_BACKEND_PRICE_HISTORY_PERIODIC_SECONDS: "3600"
    APT_BACKEND_DEFAULT_COUNTRY: "BO"
    APT_BACKEND_HTTP_ENGINE: "requests"
    APT_BACKEND_HTTP_TIMEOUT: "10.0"
    APT_BACKEND_HTTP_RETRIES: "1"
    APT_BACKEND_HTTP_MAX_CONNECTIONS: "10"
    APT_BACKEND_USER_AGENT: "Mozilla/5.0 (X11; Linux aarch64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.6613.18 Safari/537.36"
    # API limits and pagination
    APT_BACKEND_API_PAGE_SIZE_DEFAULT: "10"
    APT_BACKEND_API_PAGE_SIZE_MAX: "100"
    APT_BACKEND_API_PRICE_HISTORY_LIMIT_DEFAULT: "10"
    APT_BACKEND_API_PRICE_HISTORY_LIMIT_MAX: "200"
    APT_BACKEND_API_PRICE_HISTORY_LIMIT_FULL: "1000"
    APT_BACKEND_API_QUEUE_LIMIT_DEFAULT: "1"
    APT_BACKEND_API_QUEUE_LIMIT_MAX: "50"
    APT_BACKEND_API_BENCH_LIMIT_DEFAULT: "10"
    APT_BACKEND_API_BENCH_LIMIT_MAX: "100"
    # Database table names
    APT_BACKEND_TABLE_CURRENCIES: "currencies"
    APT_BACKEND_TABLE_PRODUCT_STATES: "product_states"
    APT_BACKEND_TABLE_PROCESS_STATES: "process_states"
    APT_BACKEND_TABLE_PRODUCT_CHECKS: "product_checks"
    APT_BACKEND_TABLE_PRICE_HISTORY: "price_history"
    APT_BACKEND_TABLE_SCREENSHOTS: "screenshots"
    APT_BACKEND_TABLE_COUNTRIES: "countries"
    APT_BACKEND_TABLE_WORKERS: "workers"
    APT_BACKEND_TABLE_BENCH: "bench"
    APT_BACKEND_TABLE_USERS: "users"
    APT_BACKEND_TABLE_USER_SESSIONS: "user_sessions"
    APT_BACKEND_TABLE_USER_TOKENS: "user_tokens"
    APT_BACKEND_TABLE_USER_PERMISSIONS: "user_permissions"
    APT_BACKEND_TABLE_PRODUCT_PROCESSING_LOGS: "product_processing_logs"
    APT_BACKEND_SCREENSHOT_ENABLED: "true"
    APT_BACKEND_SCREENSHOT_ON_PRICE_CHANGE_ONLY: "false"
    APT_BACKEND_SCREENSHOT_ON_CHANGES: "false"
    APT_BACKEND_SCREENSHOT_ALWAYS: "true"
    APT_BACKEND_SCREENSHOT_MAX_RETRIES: "3"
    APT_BACKEND_SCREENSHOT_STORAGE_PATH: "/var/screenshots"  # Mounted from global.storage (shared with CLI Job)
    APT_BACKEND_SCREENSHOT_SERVICE_URL: ""  # Auto-generated from screenshot service if empty
    APT_BACKEND_SCREENSHOT_SERVICE_WIDTH: "1200"
    APT_BACKEND_SCREENSHOT_SERVICE_HEIGHT: "960"
    APT_BACKEND_SCREENSHOT_SERVICE_FULL_PAGE: "false"
    APT_BACKEND_SCREENSHOT_SERVICE_JAVASCRIPT_ENABLED: "false"
    APT_BACKEND_SCREENSHOT_SERVICE_STOP_ON_CAPTCHA: "false"
    APT_BACKEND_SCREENSHOT_SERVICE_BROWSER_TYPE: "chromium"
    APT_BACKEND_BENCH_KEY: ""
    APT_BACKEND_TEST_MODE: "false"
  # Screenshot storage volume (deprecated - use global.storage instead)
  # This section is kept for backward compatibility but is ignored if global.storage.enabled is true
  screenshotStorage:
    enabled: false  # Deprecated: use global.storage instead
  # Pod Disruption Budget configuration
  podDisruptionBudget:
    enabled: false  # Set to true to enable PDB
    # minAvailable: 1  # Minimum number of pods that must be available (mutually exclusive with maxUnavailable)
    maxUnavailable: 1  # Maximum number of pods that can be unavailable
  # Network Policy configuration
  networkPolicy:
    enabled: false  # Set to true to enable NetworkPolicy
    # ingress: []  # Custom ingress rules (if not provided, defaults are used)
    # egress: []  # Custom egress rules (if not provided, defaults are used)
  # Argo Rollout configuration
  rollout:
    revisionHistoryLimit: 10  # Number of old ReplicaSets to retain
    progressDeadlineSeconds: 1800  # 30 minutes timeout for rollout progress
    # Deployment strategy (optional - defaults to simple canary with no steps)
    # If not specified, uses canary strategy with empty steps (rolling update behavior)
    # To enable progressive canary deployment (Upwork-style), uncomment and configure:
    strategy:
      canary:
        # Minimum pods per replica set (ensures canary has enough pods)
        minPodsPerReplicaSet: null  # Set to number (e.g., 2) if needed
        # Canary deployment steps (progressive rollout)
        # Note: setWeight requires trafficRouting (Istio/NGINX) to actually split traffic
        # Without trafficRouting, this is metadata-only for tracking progress
        steps:
        # Step 1: Set initial traffic weight (1%)
        - setWeight: 1
        # Step 2: Pause for manual verification (duration: 0 = manual pause)
        - pause:
            duration: 0  # Manual pause - requires promotion via ArgoCD UI or CLI
        # Step 3: Set traffic weight to 50%
        - setWeight: 50
        # Step 4: Pause for automated verification (2 minutes)
        - pause:
            duration: 120  # 2 minutes (120 seconds)
        # Step 5: Complete rollout (100% traffic)
        - setWeight: 100
        # Optional: Analysis templates for automated verification
        # analysis:
        #   templates:
        #   - templateName: backend-analysis
        # Optional: Traffic routing (requires Istio/Linkerd service mesh)
        # trafficRouting:
        #   istio:
        #     virtualService:
        #       name: backend
        #     destinationRule:
        #       name: backend
        #       stableSubsetName: stable
        #       canarySubsetName: canary
    # Alternative: Blue-Green deployment
    # strategy:
    #   blueGreen:
    #     activeService: backend
    #     previewService: backend-preview
    #     autoPromotionEnabled: true
  # Horizontal Pod Autoscaler configuration
  autoscaling:
    enabled: true  # Set to true to enable HPA
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80  # Optional: enable memory-based scaling
    # behavior:  # Optional: customize scaling behavior
    #   scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Percent
    #       value: 50
    #       periodSeconds: 60
    #   scaleUp:
    #     stabilizationWindowSeconds: 0
    #     policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 15

# UI Service Configuration
ui:
  enabled: true
  replicas: 1
  image:
    repository: maborak/platform
    tag: "apt-ui-0.1"
    pullPolicy: Always
  resources:
    limits:
      memory: 2Gi
      cpu: 500m
    requests:
      memory: 512Mi
      cpu: 100m
  service:
    type: ClusterIP  # Options: ClusterIP, NodePort, LoadBalancer
    # nodePort: 30080  # Only used if type is NodePort
  healthCheck:
    enabled: true
    path: /
    port: 80
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3
  # UI environment variables (VITE_* prefix)
  # All env vars with full names including prefix - no transformation needed
  env:
    VITE_API_BASE_URL: ""  # Auto-generated from backend domain if empty (uses //api.amazon-watcher.local)
    VITE_API_TIMEOUT: "30"
    VITE_CONNECTIVITY_CHECK_INTERVAL: "60"
    VITE_APP_NAME: "Amazon Watcher"
    VITE_APP_VERSION: "1.0.0"
    VITE_BUILD_DATE: ""
    VITE_PRICE_HISTORY_LIMIT: "50"
    VITE_PRICE_HISTORY_PAGE_SIZE: "10"
    VITE_DEBUG_MODE: "true"
    VITE_LOG_LEVEL: "info"
  # Pod Disruption Budget configuration
  podDisruptionBudget:
    enabled: false  # Set to true to enable PDB
    # minAvailable: 1  # Minimum number of pods that must be available (mutually exclusive with maxUnavailable)
    maxUnavailable: 1  # Maximum number of pods that can be unavailable
  # Network Policy configuration
  networkPolicy:
    enabled: false  # Set to true to enable NetworkPolicy
    # ingress: []  # Custom ingress rules (if not provided, defaults are used)
    # egress: []  # Custom egress rules (if not provided, defaults are used)
  # Argo Rollout configuration
  rollout:
    revisionHistoryLimit: 10  # Number of old ReplicaSets to retain
    progressDeadlineSeconds: 1800  # 30 minutes timeout for rollout progress
    # Deployment strategy (optional - defaults to simple canary with no steps)
    # If not specified, uses canary strategy with empty steps (rolling update behavior)
    # To enable progressive canary deployment (Upwork-style), uncomment and configure:
    strategy:
      canary:
        # Minimum pods per replica set (ensures canary has enough pods)
        minPodsPerReplicaSet: null  # Set to number (e.g., 2) if needed
        # Canary deployment steps (progressive rollout)
        # Note: setWeight requires trafficRouting (Istio/NGINX) to actually split traffic
        # Without trafficRouting, this is metadata-only for tracking progress
        steps:
        # Step 1: Set initial traffic weight (1%)
        - setWeight: 1
        # Step 2: Pause for manual verification (duration: 0 = manual pause)
        - pause:
            duration: 0  # Manual pause - requires promotion via ArgoCD UI or CLI
        # Step 3: Set traffic weight to 50%
        - setWeight: 50
        # Step 4: Pause for automated verification (2 minutes)
        - pause:
            duration: 120  # 2 minutes (120 seconds)
        # Step 5: Complete rollout (100% traffic)
        - setWeight: 100
        # Optional: Analysis templates for automated verification
        # analysis:
        #   templates:
        #   - templateName: ui-analysis
        # Optional: Traffic routing (requires Istio/Linkerd service mesh)
        # trafficRouting:
        #   istio:
        #     virtualService:
        #       name: ui
        #     destinationRule:
        #       name: ui
        #       stableSubsetName: stable
        #       canarySubsetName: canary
    # Alternative: Blue-Green deployment
    # strategy:
    #   blueGreen:
    #     activeService: ui
    #     previewService: ui-preview
    #     autoPromotionEnabled: true
  # Horizontal Pod Autoscaler configuration
  autoscaling:
    enabled: true  # Set to true to enable HPA
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80  # Optional: enable memory-based scaling
    # behavior:  # Optional: customize scaling behavior
    #   scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Percent
    #       value: 50
    #       periodSeconds: 60
    #   scaleUp:
    #     stabilizationWindowSeconds: 0
    #     policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 15

# Screenshot Service Configuration
screenshot:
  enabled: true
  replicas: 1
  image:
    repository: maborak/platform
    tag: "apt-browser-0.1"
    pullPolicy: Always
  resources:
    limits:
      memory: 2Gi
      cpu: 1000m
    requests:
      memory: 1Gi
      cpu: 500m
  service:
    type: ClusterIP  # Options: ClusterIP, NodePort, LoadBalancer
    # nodePort: 30300  # Only used if type is NodePort
  healthCheck:
    enabled: true
    path: /health
    port: 3000
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3
  # Argo Rollout configuration
  rollout:
    revisionHistoryLimit: 10  # Number of old ReplicaSets to retain
    progressDeadlineSeconds: 1800  # 30 minutes timeout for rollout progress
    # Deployment strategy (optional - defaults to simple canary with no steps)
    # If not specified, uses canary strategy with empty steps (rolling update behavior)
    # To enable progressive canary deployment (Upwork-style), uncomment and configure:
    strategy:
      canary:
        # Minimum pods per replica set (ensures canary has enough pods)
        minPodsPerReplicaSet: null  # Set to number (e.g., 2) if needed
        # Canary deployment steps (progressive rollout)
        # Note: setWeight requires trafficRouting (Istio/NGINX) to actually split traffic
        # Without trafficRouting, this is metadata-only for tracking progress
        steps:
        # Step 1: Set initial traffic weight (1%)
        - setWeight: 1
        # Step 2: Pause for manual verification (duration: 0 = manual pause)
        - pause:
            duration: 0  # Manual pause - requires promotion
        # Step 3: Set traffic weight to 50%
        - setWeight: 50
        # Step 4: Pause for automated verification (2 minutes)
        - pause:
            duration: 120  # 2 minutes
        # Step 5: Complete rollout (100% traffic)
        - setWeight: 100
        # Optional: Analysis templates for automated verification
        # analysis:
        #   templates:
        #   - templateName: screenshot-analysis
        # Optional: Traffic routing (requires Istio/Linkerd service mesh)
        # trafficRouting:
        #   istio:
        #     virtualService:
        #       name: screenshot
        #     destinationRule:
        #       name: screenshot
        #       stableSubsetName: stable
        #       canarySubsetName: canary
    # Alternative: Blue-Green deployment
    # strategy:
    #   blueGreen:
    #     activeService: screenshot
    #     previewService: screenshot-preview
    #     autoPromotionEnabled: true
  # Horizontal Pod Autoscaler configuration
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: null  # Set to enable memory-based scaling
    # Optional: Custom scaling behavior
    # behavior:
    #   scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Percent
    #       value: 50
    #       periodSeconds: 60
    #   scaleUp:
    #     stabilizationWindowSeconds: 0
    #     policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 60
    #     - type: Pods
    #       value: 2
    #       periodSeconds: 60
    #     selectPolicy: Max
  # Browser environment variables (APT_BROWSER_* prefix)
  # All env vars with full names including prefix - no transformation needed
  env:
    APT_BROWSER_HOST: "0.0.0.0"
    APT_BROWSER_PORT: "3000"
    APT_BROWSER_RELOAD: "false"
    APT_BROWSER_WORKERS: "1"
    APT_BROWSER_LOG_LEVEL: "INFO"
    APT_BROWSER_ACCESS_LOG: "true"
    APT_BROWSER_LIMIT_CONCURRENCY: "100"
    APT_BROWSER_LIMIT_MAX_REQUESTS: "1000"
    APT_BROWSER_TIMEOUT_KEEP_ALIVE: "5"
    APT_BROWSER_TIMEOUT_GRACEFUL_SHUTDOWN: "30"
    APT_BROWSER_BROWSER_HEADLESS: "true"
    APT_BROWSER_BROWSER_TIMEOUT: "30000"
    APT_BROWSER_BROWSER_IDLE_TIMEOUT: "300"
    APT_BROWSER_MAX_CONCURRENT: "100"
    APT_BROWSER_REQUEST_TIMEOUT: "120"
    APT_BROWSER_DEFAULT_WIDTH: "1920"
    APT_BROWSER_DEFAULT_HEIGHT: "1080"
    APT_BROWSER_DEFAULT_FULL_PAGE: "false"
    APT_BROWSER_DEFAULT_WAIT_UNTIL: "networkidle"
    APT_BROWSER_DEFAULT_MAX_RETRIES: "3"
    APT_BROWSER_DEFAULT_STOP_ON_CAPTCHA: "false"
    APT_BROWSER_DEFAULT_JAVASCRIPT_ENABLED: "true"
    APT_BROWSER_DEFAULT_BROWSER_TYPE: "chromium"
    APT_BROWSER_CAPTCHA_SCROLL_STEP: "500"
    APT_BROWSER_CAPTCHA_RETRY_DELAY_MIN: "2.0"
    APT_BROWSER_CAPTCHA_RETRY_DELAY_MAX: "4.0"
    APT_BROWSER_NAVIGATION_MAX_RETRIES: "3"
    APT_BROWSER_USER_AGENT: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36"
  # Pod Disruption Budget configuration
  podDisruptionBudget:
    enabled: false  # Set to true to enable PDB
    # minAvailable: 1  # Minimum number of pods that must be available (mutually exclusive with maxUnavailable)
    maxUnavailable: 1  # Maximum number of pods that can be unavailable
  # Network Policy configuration
  networkPolicy:
    enabled: false  # Set to true to enable NetworkPolicy
    # ingress: []  # Custom ingress rules (if not provided, defaults are used)
    # egress: []  # Custom egress rules (if not provided, defaults are used)

# Istio Service Mesh Configuration
# Required for Argo Rollouts traffic splitting with canary deployments
istio:
  enabled: false  # Set to true if Istio is installed in your cluster
  virtualService:
    enabled: false  # Set to true to create VirtualService for traffic routing
    # Custom routes (if not provided, default routes for Rollout canary are used)
    # routes: []
  destinationRule:
    enabled: false  # Set to true to create DestinationRule for canary subsets
    # Custom traffic policy (optional)
    # trafficPolicy: {}
  telemetry:
    enabled: false  # Set to true to create Telemetry resources for observability
    # Custom metrics configuration (optional)
    # metrics: []
    # Custom access logging (optional)
    # accessLogging: []
    # Custom tracing (optional)
    # tracing: []

# Kubernetes Ingress Configuration
# Works with any ingress controller (NGINX, Traefik, etc.)
# Note: Ingress resources are only created if both enabled=true AND className is set
ingress:
  enabled: false
  className: "nginx"  # Set to your ingress controller class (e.g., "nginx", "traefik"). If empty, Ingress resources will not be created.
  
  # TLS/SSL Configuration
  tls:
    enabled: false
    # If enabled, you need to provide TLS certificates
    # You can use cert-manager for automatic certificate management
    # or manually create TLS secrets

# Secrets (sensitive values)
secrets:
  # Database password (if not using database.postgres.password)
  databasePassword: ""
  # JWT secret for backend
  jwtSecret: ""

# Service Account
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Pod Security Context
podSecurityContext:
  fsGroup: 2000
  runAsNonRoot: true
  runAsUser: 1000

# Security Context
securityContext:
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  allowPrivilegeEscalation: false

# Node Selector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity
affinity: {}

# Annotations
podAnnotations: {}

# Labels
podLabels: {}

# ArgoCD Configuration
argocd:
  # ArgoCD annotations for sync control and metadata
  annotations:
    # Application metadata (similar to Upwork pattern)
    app.maborak.com/contact: ""  # e.g., "team@maborak.com"
    app.maborak.com/created-by: "argo-cd"  # Indicates ArgoCD created this resource
    app.maborak.com/department: ""  # e.g., "platform", "engineering"
    app.maborak.com/project: ""  # e.g., "amazon-watcher"
    
    # Argo Rollouts notifications (Slack, etc.)
    # notifications.argoproj.io/subscribe.on-rollout-aborted.slack: "channel-name"
    # notifications.argoproj.io/subscribe.on-rollout-completed.slack: "channel-name"
    # notifications.argoproj.io/subscribe.on-rollout-step-completed.slack: "channel-name"
    # notifications.argoproj.io/subscribe.on-rollout-updated.slack: "channel-name"
    
    # Sync control annotations
    # argocd.argoproj.io/sync-wave: "0"  # Lower numbers sync first
    # argocd.argoproj.io/sync-options: "Prune=false"  # Comma-separated options
    # argocd.argoproj.io/hook: "PreSync"  # Hook type: PreSync, PostSync, Sync, Skip
    # argocd.argoproj.io/hook-weight: "0"  # Hook execution order
    # argocd.argoproj.io/refresh: "hard"  # Refresh mode: normal, hard
    # argocd.argoproj.io/compare-options: "IgnoreExtraneous"  # Compare options
    
  # Application labels (similar to Upwork pattern)
  labels:
    # Standard Kubernetes labels (already included in common labels)
    # app.kubernetes.io/name, app.kubernetes.io/instance, app.kubernetes.io/component
    # app.kubernetes.io/managed-by, app.kubernetes.io/version, app.kubernetes.io/part-of
    
    # Organization-specific labels
    app.maborak.com/bounded-context: ""  # Domain/bounded context
    app.maborak.com/chart-name: "amazon-watcher-stack"  # Helm chart name
    app.maborak.com/cluster-name: ""  # Kubernetes cluster name
    app.maborak.com/environment: "Development"  # Environment: prod, staging, dev
    app.maborak.com/family: ""  # Technology family: backend, frontend, database
    app.maborak.com/runtime: ""  # Runtime: eks, gke, aks, docker-desktop
    app.maborak.com/service-runtime: ""  # Service runtime: nodejs, python, go
    app.maborak.com/team: ""  # Team name
    app.maborak.com/tier: ""  # Application tier: T1, T2, T3
    app.maborak.com/type: ""  # Application type: business, infrastructure, platform
    
    # ArgoCD instance label
    argocd.argoproj.io/instance: ""  # ArgoCD application instance name
    
    # Platform-specific labels
    platform.maborak.com/namespace: ""  # Target namespace
    # platform.maborak.com/network-policies-version: ""
    # platform.maborak.com/istio-telemetry-api-flag: ""
    
  # Legacy tags (deprecated - use labels instead, but kept for backward compatibility)
  tags:
    environment: "Development"  # Maps to app.maborak.com/environment
    team: ""  # Maps to app.maborak.com/team
    project: ""  # Maps to app.maborak.com/project
